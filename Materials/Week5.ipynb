{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5864d3f2-85b4-46ea-acb1-479d7aff3623",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Week 5 - Contrastive Language–Image Pretraining（CLIP） </h1>\n",
    "\n",
    "<p style=\"text-align:center\">汇报人：xxx</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96740a43-2276-4315-96c1-a5c7f1454dc6",
   "metadata": {},
   "source": [
    "\n",
    "###  建议学习资料：\n",
    "\n",
    "- [论文原文：Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "- [CLIP 论文逐段精读【论文精读】](www.bilibili.com/video/BV1SL4y1s7LQ)\n",
    "- [多模态模型CLIP深度讲解](www.bilibili.com/video/BV1pYmDYgEDW)\n",
    "- [OpenAI 官方实验代码](https://github.com/openai/CLIP)\n",
    "\n",
    "###  本周任务清单：\n",
    "\n",
    "1.  **理论学习**\n",
    "   - 理解多模态模型的核心思想，CLIP 模型的基本原理与架构  \n",
    "\n",
    "2.  **实践任务**\n",
    "   - 在 CIFAR10 数据集上，使用 CLIP 模型完成 zero-shot 分类\n",
    "   - 对比 CLIP 与单模态 ResNet的性能   \n",
    "\n",
    "3.  **提交要求**\n",
    "   - 提交一个 `.ipynb` 文件，文件内容需包括：\n",
    "     - 每个理论学习点下方填写的你整理的学习内容（可结合简单代码辅助理解）\n",
    "     - 所有实践任务的完整代码实现与运行（包含适当注释）\n",
    "   - 注：对文字排版无具体要求；使用 Jupyter Notebook 打开，图片可直接通过粘贴插入 Markdown  \n",
    "   - 文件命名格式：`Week5_姓名.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b5b05-65c8-4026-b7e8-44eb556c182f",
   "metadata": {},
   "source": [
    "### 理论学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96462462-a05f-4cc9-ab87-e3ee40a008fd",
   "metadata": {},
   "source": [
    "#### 1. CLIP 背景介绍\n",
    "- CLIP 的提出背景：OpenAI 对大规模图文数据的探索\n",
    "- 与传统视觉模型的区别：无需特定任务监督即可实现 zero-shot 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707194c-cb2a-4fed-9185-8a3c81563ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa8da893-4131-4d40-a8be-9d5755eb497d",
   "metadata": {},
   "source": [
    "#### 2. 模型架构\n",
    "- 双塔结构：一个图像编码器（如 ResNet/ViT）+ 一个文本编码器（如 Transformer）\n",
    "- 对比损失：图文对之间最大化相似度，错配对最小化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904868ce-87e6-4629-beca-80c1a6b1b2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1357d3be-41c5-4069-8153-d7d0686b429f",
   "metadata": {},
   "source": [
    "#### 3. 训练策略\n",
    "- 了解CLIP的训练方法\n",
    "- 损失函数：对比学习中的 InfoNCE 损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eadc97-d592-4568-9443-ad473c5530cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c316e85d-93ee-446c-ae92-3b72695f7ebe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36822b02-cb97-4d9e-b8ba-fc733807e20d",
   "metadata": {},
   "source": [
    "### 实践任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c49b98-ed4e-4548-aa08-ff9dcceb45cc",
   "metadata": {},
   "source": [
    "#### 任务一：在 CIFAR10 数据集上，使用 CLIP 模型完成 zero-shot 分类\n",
    "\n",
    "**目标：** 利用预训练的 CLIP 模型进行 zero-shot 图像分类任务，在 CIFAR10 数据集上评估其分类性能。\n",
    "\n",
    "**要求：**\n",
    "- 模型加载：\n",
    "  - 使用 HuggingFace 或 OpenAI 官方提供的 CLIP 模型代码加载预训练模型。\n",
    "  - 熟悉模型文档，确保理解 CLIP 的输入预处理和文本 prompt 构造方法。\n",
    "- 数据准备：\n",
    "  - 下载CIFAR10数据集\n",
    "  - 按照 CLIP 模型要求对图像进行预处理（如 resize、归一化、中心裁剪等）。\n",
    "- 分类方法：\n",
    "  - 为每个类别构造文本 prompt（例如 “a photo of a {label}”）。\n",
    "  - 将所有类别的文本 prompt 通过 CLIP 的文本编码器得到文本特征。\n",
    "  - 利用 CLIP 的图像编码器得到图像特征，计算图像特征与各类别文本特征之间的相似度。\n",
    "  - 根据相似度得分进行排序，作为模型的预测标签（即分类结果）。\n",
    "- 评估指标：\n",
    "  - 计算在 CIFAR100 数据集上的 Top-1 与 Top-5 分类准确率。\n",
    "  - 可选：计算 confusion matrix、PR/ROC 曲线等辅助指标。\n",
    "- 可视化与分析：\n",
    "  - 展示部分示例图像、对应的文本 prompt 以及模型输出的相似度得分分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b96656-9de7-4dbd-89f2-3ba157cf0c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c86766-9e49-44ba-8b3f-f303f06be100",
   "metadata": {},
   "source": [
    "#### 任务二：对比 CLIP 与单模态 ResNet 模型在图像分类任务中的性能差异\n",
    "\n",
    "**目标：** 通过实验比较预训练的多模态模型 CLIP 与单模态模型 ResNet 在图像分类任务中的表现差异，分析多模态模型在图像理解中的潜在优势。\n",
    "\n",
    "**要求：**\n",
    "- 模型准备：\n",
    "  - 加载 CLIP 模型（使用任务一中配置，基于 zero-shot 分类方式）。\n",
    "  - 自行搭建ResNet模型。\n",
    "  - 理解两种模型的输入预处理流程和输出结构。\n",
    "- 数据准备：\n",
    "  - 自行选择数据集。\n",
    "- 评估指标：\n",
    "  - 分别计算 CLIP 与 ResNet 模型在数据集上的 Top-1 和 Top-5 分类准确率。\n",
    "  - 可选：绘制 confusion matrix、每类准确率条形图等用于深入分析的指标。\n",
    "- 可视化与分析：\n",
    "  - 可视化两个模型在相同图像上的预测结果，列出成功预测与失败预测的样例。\n",
    "  - 多方面比较两种方法的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5afa24-e514-442e-990f-bc9c4e6eba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
