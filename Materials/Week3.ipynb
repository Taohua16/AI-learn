{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554a2ab3-0de9-4833-a229-454c095964b8",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Week 3 - RNN </h1>\n",
    "\n",
    "<p style=\"text-align:center\">汇报人：xxx</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ce4d9-c3bc-49fa-b5ac-0915b22c0c64",
   "metadata": {},
   "source": [
    "\n",
    "###  建议学习资料：\n",
    "\n",
    "- [李沐动手学深度学习54-59](https://space.bilibili.com/1567748478/lists/358497?type=series)\n",
    "\n",
    "\n",
    "###  本周任务清单：\n",
    "\n",
    "1.  **理论学习**\n",
    "   - 学习循环神经网络（RNN）的基本原理  \n",
    "   - 学习长短期记忆网络（LSTM）的基本原理  \n",
    "   - 学习门控循环单元（GRU）的基本原理  \n",
    "   - 对比 RNN、LSTM 与 GRU\n",
    "\n",
    "2.  **实践任务**\n",
    "   - 分别使用 RNN、LSTM 和 GRU 模型，在 RML2016 数据集上进行信号分类  \n",
    "   - 比较三种模型（RNN / LSTM / GRU）在准确率、训练速度、收敛曲线等方面的表现  \n",
    "\n",
    "3.  **提交要求**\n",
    "   - 提交一个 `.ipynb` 文件，文件内容需包括：\n",
    "     - 每个理论学习点下方填写的你整理的学习内容（可结合简单代码辅助理解）\n",
    "     - 所有实践任务的结果展示（简单的实践任务代码可直接写在该文档）\n",
    "   - 注：对文字排版无具体要求；使用 Jupyter Notebook 打开，图片可直接通过粘贴插入 Markdown  \n",
    "   - 文件命名格式：`Week3_姓名.ipynb`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c78e38-f670-495f-b9b4-59ae708e52f8",
   "metadata": {},
   "source": [
    "### 理论学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89f33b-43f3-47d5-af57-eaa8a74fb735",
   "metadata": {},
   "source": [
    "#### 1. 循环神经网络（RNN）的基本原理  \n",
    "- 掌握 RNN 的基本原理  \n",
    "- 了解 RNN 存在的梯度消失与梯度爆炸问题  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87018bd6-4a60-4a81-b0d6-72cdae585164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca3feb8a-eb51-43ea-b003-4199566e2813",
   "metadata": {},
   "source": [
    "#### 2. 长短期记忆网络（LSTM）的基本原理  \n",
    "- 理解 LSTM 的核心结构：输入门、遗忘门、输出门  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04b3e0-bfb9-478c-9742-51e55d43b3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb7180a-564a-4d73-b49c-0c0ba13b696e",
   "metadata": {},
   "source": [
    "#### 3. 门控循环单元（GRU）的基本原理  \n",
    "- 理解 GRU 的核心结构：重置门与更新门\n",
    "- 对比 GRU 与 LSTM 的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f011328-f2d6-43f9-8c5c-a73b27b76083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83516af-6038-4036-8768-d73ebddca5cd",
   "metadata": {},
   "source": [
    "#### 4. RNN、LSTM 与 GRU 的对比  \n",
    "- 对比三种结构在建模能力、训练速度和参数量上的差异  \n",
    "- 分析三者在不同任务（如文本、语音）中的适用场景  \n",
    "- 掌握如何根据任务需求选择合适的循环神经网络结构  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86d7c6-d989-42cb-a93a-94ec7ca1658d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0215847-9561-4c20-99ce-4dd2503b3fd3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b5270-f7b9-43cf-89fa-0b342c0c3586",
   "metadata": {},
   "source": [
    "### 实践任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3621b-44fb-4ec9-885b-caf61b9f4ca8",
   "metadata": {},
   "source": [
    "#### 任务一：构建一个简单 RNN / LSTM / GRU 模型（用于调制识别）\n",
    "\n",
    "**目标：** 使用 PyTorch 构建三个基础的循环神经网络模型（RNN、LSTM 和 GRU），对调制信号数据进行分类识别任务，并比较它们在该任务中的表现差异。\n",
    "\n",
    "**要求：**\n",
    "\n",
    "- 下载[RML2016.10a数据集](https://www.kaggle.com/datasets/raindrops12/rml201610a),使用提供的数据加载代码（输入数据维度为 `[batch_size, 2, 128]`，即2个通道、128个时间步）\n",
    "- 分别构建以下三种模型结构：RNN 模型、LSTM 模型、GRU 模型。三者的整体结构一致，仅循环单元不同，具体结构如下（可自行修改，尝试提高准确率）：\n",
    "  - 嵌入层（Embedding）：使用 `nn.Conv1d(2, 64, kernel_size=16, stride=8)` \n",
    "  - 循环层1：输入维度64，隐藏单元128\n",
    "  - 循环层2：输入维度128，隐藏单元128\n",
    "  - 分别使用 `nn.RNN`、`nn.LSTM` 和 `nn.GRU` 构建对应模型\n",
    "  - 全连接层：输入128维，输出11类（使用序列的最后一个时间步的输出进行分类）\n",
    "- 损失函数：`nn.CrossEntropyLoss()`\n",
    "- 优化器：`Adam`\n",
    "- 学习率：初始学习率设为 `0.001`，验证集准确率每3个epoch不下降，学习率x0.5\n",
    "- EarlyStop：`patience=10`\n",
    "- 可视化训练过程中的 `loss` 和 `accuracy` 曲线\n",
    "- 输出模型在训练集与测试集上的分类准确率\n",
    "- 对 RNN、LSTM、GRU 三种结构的运行时间、预测性能进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b8b4863-3253-4b4a-ae6c-2ae38c804be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def to_num(y):\n",
    "    label = []\n",
    "    classes = ['8PSK','AM-DSB','AM-SSB','BPSK','CPFSK','GFSK','PAM4','QAM16','QAM64','QPSK', 'WBFM']\n",
    "    for j in range(len(y)):\n",
    "        label.append(classes.index(y[j]))\n",
    "    label = torch.tensor(label)\n",
    "    return label\n",
    "\n",
    "def RML_dataloader(train_rate, valid_rate, test_rate, batch_size=64, random_seed=2024, num_workers=2):\n",
    "\n",
    "    data = pd.read_pickle('RML2016.10a_dict.pkl')\n",
    "    snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], data.keys())))), [1, 0])\n",
    "    X = []\n",
    "    label = []\n",
    "    for mod in mods:\n",
    "        for snr in snrs:\n",
    "            X.append(data[(mod, snr)])\n",
    "            for i in range(data[(mod, snr)].shape[0]):  label.append((mod, snr))\n",
    "    X = np.vstack(X)\n",
    "\n",
    "    n_examples = X.shape[0]\n",
    "    n_train = int(train_rate * n_examples)\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    Slices_list = np.linspace(0, n_examples, num=len(mods) * len(snrs) + 1)\n",
    "\n",
    "    for k in range(0, Slices_list.shape[0] - 1):\n",
    "        train_idx_subset = np.random.choice(\n",
    "            range(int(Slices_list[k]), int(Slices_list[k + 1])), size=int(n_train / (len(mods) * len(snrs))),\n",
    "            replace=False)\n",
    "        Test_Val_idx_subset = list(set(range(int(Slices_list[k]), int(Slices_list[k + 1]))) - set(train_idx_subset))\n",
    "        test_idx_subset = np.random.choice(Test_Val_idx_subset,\n",
    "                                           size=int(\n",
    "                                               (n_examples - n_train) * test_rate / (\n",
    "                                                       (len(mods) * len(snrs)) * (test_rate + valid_rate))),\n",
    "                                           replace=False)\n",
    "        val_idx_subset = list(set(Test_Val_idx_subset) - set(test_idx_subset))\n",
    "\n",
    "        train_idx = np.hstack([train_idx, train_idx_subset])\n",
    "        val_idx = np.hstack([val_idx, val_idx_subset])\n",
    "        test_idx = np.hstack([test_idx, test_idx_subset])\n",
    "\n",
    "    train_indices = train_idx.astype('int64')\n",
    "    val_indices = val_idx.astype('int64')\n",
    "    test_indices = test_idx.astype('int64')\n",
    "\n",
    "    X_train = torch.tensor(X[train_indices])  # [N, 2, 128]\n",
    "    X_val = torch.tensor(X[val_indices])\n",
    "    X_test = torch.tensor(X[test_indices])\n",
    "    y_train, y_val, y_test = [], [], []\n",
    "    snr_train, snr_val, snr_test = [], [], []\n",
    "\n",
    "    for i in train_indices:\n",
    "        y_train.append(label[i][0])\n",
    "        snr_train.append(label[i][1])\n",
    "    for i in val_indices:\n",
    "        y_val.append(label[i][0])\n",
    "        snr_val.append(label[i][1])\n",
    "    for i in test_indices:\n",
    "        y_test.append(label[i][0])\n",
    "        snr_test.append(label[i][1])\n",
    "\n",
    "    y_train = to_num(y_train)\n",
    "    y_val = to_num(y_val)\n",
    "    y_test = to_num(y_test)\n",
    "\n",
    "    snr_test = torch.tensor(snr_test)\n",
    "    snr_val = torch.tensor(snr_val)\n",
    "    snr_train = torch.tensor(snr_train)\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_train, y_train, snr_train)\n",
    "    loader1 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_val, y_val, snr_val)\n",
    "    loader2 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_test, y_test, snr_test)\n",
    "    loader3 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return loader1, loader2, loader3  # train,valid,test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_loader, valid_loader, test_loader = RML_dataloader(0.6, 0.2, 0.2, batch_size=500, random_seed=2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f16e4-90c4-47e5-856e-17fedf5cef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
