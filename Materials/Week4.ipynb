{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5864d3f2-85b4-46ea-acb1-479d7aff3623",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Week 4 - Transformer </h1>\n",
    "\n",
    "<p style=\"text-align:center\">汇报人：xxx</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f8938-4dc5-415f-b5ad-8a497e6801b7",
   "metadata": {},
   "source": [
    "\n",
    "###  建议学习资料：\n",
    "\n",
    "- [李沐动手学深度学习61-68](https://space.bilibili.com/1567748478/lists/358497?type=series)\n",
    "- [原文Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "###  本周任务清单：\n",
    "\n",
    "1.  **理论学习**\n",
    "   - 学习 Transformer 模型的基本原理  \n",
    "\n",
    "2.  **实践任务**\n",
    "   - 构建一个 CNN+Transformer 模型，在 RML2016.10a 数据集上进行信号分类\n",
    "   - 构建一个 纯Transformer 模型，在 RML2016.10a 数据集上进行信号分类\n",
    "   - 比较两种模型训练的差异\n",
    "\n",
    "3.  **提交要求**\n",
    "   - 提交一个 `.ipynb` 文件，文件内容需包括：\n",
    "     - 每个理论学习点下方填写的你整理的学习内容（可结合简单代码辅助理解）\n",
    "     - 所有实践任务的结果展示（简单的实践任务代码可直接写在该文档）\n",
    "   - 注：对文字排版无具体要求；使用 Jupyter Notebook 打开，图片可直接通过粘贴插入 Markdown  \n",
    "   - 文件命名格式：`Week4_姓名.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b5b05-65c8-4026-b7e8-44eb556c182f",
   "metadata": {},
   "source": [
    "### 理论学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8da893-4131-4d40-a8be-9d5755eb497d",
   "metadata": {},
   "source": [
    "#### 1. 位置编码（Positional Encoding）\n",
    "- 理解位置编码的作用：在 Transformer 中，如何通过位置编码为模型引入序列的位置信息\n",
    "- 位置编码的两种常见方式：正弦函数（Sinusoidal Encoding）与可学习位置编码（Learned Positional Encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904868ce-87e6-4629-beca-80c1a6b1b2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1357d3be-41c5-4069-8153-d7d0686b429f",
   "metadata": {},
   "source": [
    "#### 2. (多头)自注意力机制（Attention Mechanism）\n",
    "- 理解自注意力机制（Self-Attention）如何通过计算查询（Query）、键（Key）和值（Value）来生成加权表示\n",
    "- 学习点积注意力（Scaled Dot-Product Attention）的原理和公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eadc97-d592-4568-9443-ad473c5530cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd90556-7e18-4c7f-9523-c84d49e3ec0e",
   "metadata": {},
   "source": [
    "#### 3. 交叉注意力机制（Cross-Attention Mechanism）\n",
    "- 理解交叉注意力机制的概念，特别是在Transformer 解码器中，如何通过交叉注意力机制对编码器的输出进行加权\n",
    "- 对比交叉注意力与自注意力的差异，理解它们各自的应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8542b-973b-455b-a0e5-8b64d162d789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "309472f0-6dc4-4993-aa95-9c17cfdcf790",
   "metadata": {},
   "source": [
    "#### 4. Transformer 编码器与解码器\n",
    "- 了解 Transformer 中编码器（Encoder）与解码器（Decoder）的工作原理和结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707194c-cb2a-4fed-9185-8a3c81563ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8444519e-b44f-4869-8898-2f2ffe87e159",
   "metadata": {},
   "source": [
    "#### 5. 残差连接与层归一化（Residual Connection & Layer Normalization）\n",
    "- 理解 Transformer 中的残差连接如何帮助缓解梯度消失问题，提升训练稳定性\n",
    "- 学习层归一化的原理及其在 Transformer 中的应用，确保网络的训练能够更加平稳\n",
    "- 对比Batch Normalization 和 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57893a46-290a-4c26-b761-ccf900c86fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c316e85d-93ee-446c-ae92-3b72695f7ebe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36822b02-cb97-4d9e-b8ba-fc733807e20d",
   "metadata": {},
   "source": [
    "### 实践任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c49b98-ed4e-4548-aa08-ff9dcceb45cc",
   "metadata": {},
   "source": [
    "#### 任务一：构建一个 CNN+Transformer 模型（用于调制识别）\n",
    "\n",
    "**目标：** 使用 PyTorch 构建CNN+Transformer 模型，对调制信号数据进行分类识别任务。\n",
    "\n",
    "**要求：**\n",
    "\n",
    "- **自行学习Transforemr代码**\n",
    "- 使用提供的数据加载代码（输入数据维度为 `[batch_size, 2, 128]`，即2个通道、128个时间步）\n",
    "- 模型部分自行设计：\n",
    "  - CNN：参考ResNet自行设计(CNN提取到的特征怎么转化为适合Transformer的输入自行调整)\n",
    "  - Transformer：d_model=128, num_heads=8, hidden_dim=256, num_layers=2（仅供参考）\n",
    "  - 全连接层：输入128维，输出11类（使用平均操作得到全局特征 或者 class token代表全局特征进行分类）\n",
    "- 损失函数：`nn.CrossEntropyLoss()`\n",
    "- 优化器、学习率、EarlyStop：自行设计\n",
    "- 可视化训练过程中的 `loss` 和 `accuracy` 曲线\n",
    "- 输出模型在训练集与测试集上的分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e40a9-a7c7-441d-8d07-4623725ef60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def to_num(y):\n",
    "    label = []\n",
    "    classes = ['8PSK','AM-DSB','AM-SSB','BPSK','CPFSK','GFSK','PAM4','QAM16','QAM64','QPSK', 'WBFM']\n",
    "    for j in range(len(y)):\n",
    "        label.append(classes.index(y[j]))\n",
    "    label = torch.tensor(label)\n",
    "    return label\n",
    "\n",
    "def RML_dataloader(train_rate, valid_rate, test_rate, batch_size=64, random_seed=2024, num_workers=2):\n",
    "\n",
    "    data = pd.read_pickle('RML2016.10a_dict.pkl')\n",
    "    snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], data.keys())))), [1, 0])\n",
    "    X = []\n",
    "    label = []\n",
    "    for mod in mods:\n",
    "        for snr in snrs:\n",
    "            X.append(data[(mod, snr)])\n",
    "            for i in range(data[(mod, snr)].shape[0]):  label.append((mod, snr))\n",
    "    X = np.vstack(X)\n",
    "\n",
    "    n_examples = X.shape[0]\n",
    "    n_train = int(train_rate * n_examples)\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    Slices_list = np.linspace(0, n_examples, num=len(mods) * len(snrs) + 1)\n",
    "\n",
    "    for k in range(0, Slices_list.shape[0] - 1):\n",
    "        train_idx_subset = np.random.choice(\n",
    "            range(int(Slices_list[k]), int(Slices_list[k + 1])), size=int(n_train / (len(mods) * len(snrs))),\n",
    "            replace=False)\n",
    "        Test_Val_idx_subset = list(set(range(int(Slices_list[k]), int(Slices_list[k + 1]))) - set(train_idx_subset))\n",
    "        test_idx_subset = np.random.choice(Test_Val_idx_subset,\n",
    "                                           size=int(\n",
    "                                               (n_examples - n_train) * test_rate / (\n",
    "                                                       (len(mods) * len(snrs)) * (test_rate + valid_rate))),\n",
    "                                           replace=False)\n",
    "        val_idx_subset = list(set(Test_Val_idx_subset) - set(test_idx_subset))\n",
    "\n",
    "        train_idx = np.hstack([train_idx, train_idx_subset])\n",
    "        val_idx = np.hstack([val_idx, val_idx_subset])\n",
    "        test_idx = np.hstack([test_idx, test_idx_subset])\n",
    "\n",
    "    train_indices = train_idx.astype('int64')\n",
    "    val_indices = val_idx.astype('int64')\n",
    "    test_indices = test_idx.astype('int64')\n",
    "\n",
    "    X_train = torch.tensor(X[train_indices])  # [N, 2, 128]\n",
    "    X_val = torch.tensor(X[val_indices])\n",
    "    X_test = torch.tensor(X[test_indices])\n",
    "    y_train, y_val, y_test = [], [], []\n",
    "    snr_train, snr_val, snr_test = [], [], []\n",
    "\n",
    "    for i in train_indices:\n",
    "        y_train.append(label[i][0])\n",
    "        snr_train.append(label[i][1])\n",
    "    for i in val_indices:\n",
    "        y_val.append(label[i][0])\n",
    "        snr_val.append(label[i][1])\n",
    "    for i in test_indices:\n",
    "        y_test.append(label[i][0])\n",
    "        snr_test.append(label[i][1])\n",
    "\n",
    "    y_train = to_num(y_train)\n",
    "    y_val = to_num(y_val)\n",
    "    y_test = to_num(y_test)\n",
    "\n",
    "    snr_test = torch.tensor(snr_test)\n",
    "    snr_val = torch.tensor(snr_val)\n",
    "    snr_train = torch.tensor(snr_train)\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_train, y_train, snr_train)\n",
    "    loader1 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_val, y_val, snr_val)\n",
    "    loader2 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(X_test, y_test, snr_test)\n",
    "    loader3 = Data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return loader1, loader2, loader3  # train,valid,test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_loader, valid_loader, test_loader = RML_dataloader(0.6, 0.2, 0.2, batch_size=500, random_seed=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b96656-9de7-4dbd-89f2-3ba157cf0c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c86766-9e49-44ba-8b3f-f303f06be100",
   "metadata": {},
   "source": [
    "#### 任务二：构建一个 纯Transformer 模型（用于调制识别）\n",
    "\n",
    "**目标：** 使用 PyTorch 构建 Transformer 模型，对调制信号数据进行分类识别任务。\n",
    "\n",
    "**要求：**\n",
    "\n",
    "- 使用提供的数据加载代码（输入数据维度为 `[batch_size, 2, 128]`，即2个通道、128个时间步）\n",
    "- 模型部分自行设计：\n",
    "  - 嵌入层（Embedding）：使用 `nn.Conv1d(2, 64, kernel_size=16, stride=8)` \n",
    "  - Transformer：d_model=128, num_heads=8, hidden_dim=256, num_layers=2（仅供参考）\n",
    "  - 全连接层：输入128维，输出11类（使用平均操作得到全局特征 或者 class token代表全局特征进行分类）\n",
    "- 损失函数：`nn.CrossEntropyLoss()`\n",
    "- 优化器、学习率、EarlyStop：自行设计\n",
    "- 可视化训练过程中的 `loss` 和 `accuracy` 曲线\n",
    "- 输出模型在训练集与测试集上的分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5afa24-e514-442e-990f-bc9c4e6eba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131ba5f4-5c13-4298-9972-f2eede59e6d1",
   "metadata": {},
   "source": [
    "#### 任务三：结果比较\n",
    "\n",
    "**目标：** 比较上述两种架构在训练过程中、训练结果上有何不同。\n",
    "\n",
    "**要求：**\n",
    "\n",
    "- 多方面比较二者区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381291e-dbb7-4a43-be7d-a05f5b6de9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
